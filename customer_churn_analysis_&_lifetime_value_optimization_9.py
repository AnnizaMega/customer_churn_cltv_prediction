# -*- coding: utf-8 -*-
"""Customer Churn Analysis & Lifetime Value Optimization-9.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19Cy5IYObDixmQAw6FmYI7tDNhUKaEiLg
"""

r6from google.colab import auth, drive
from oauth2client.client import GoogleCredentials
import pandas as pd
import io

# auth.authenticate_user()
drive.mount('/content/drive')

from google.colab import drive
drive.mount('/content/drive')

df = pd.read_csv('/content/drive/My Drive/Colab Notebooks/churn.csv')

"""#EDA"""

df.head()

# Get the size of the dataset
df.shape

# Get list of columns and their data types
df.info()

# Get summary statistics of the dataset
df.describe()

# Count unique values in each column
for col in df.columns:
  print(col, df[col].nunique())

# Isolate the categorical variables
categorical_variables = [col for col in df.columns if df[col].nunique() <= 11 and col not in "Exited"]
categorical_variables

# Isolate the numerical variables
numerical_variables = [col for col in df.columns if df[col].dtype != "object"
                        and df[col].nunique() > 11
                        and col not in "CustomerId"
                        and col not in "RowNumber"]
numerical_variables

# Get the count of values of the dependent variable
df["Exited"].value_counts()

"""Churn in the dataset (Exited = 1) only makes ~20% of the dataset.
This is a clear imbalanced data that can affect the performance of the prediction model.
"""

# Customers who left the bank
churn = df.loc[df["Exited"]==1]

# Customers who did not leave the bank
not_churn = df.loc[df["Exited"]==0]

# Check Age distribution
import matplotlib.pyplot as plt

plt.hist(not_churn["Age"], label="Not Churn")
plt.hist(churn["Age"], label="Churn")
plt.title("Distribution of Age")
plt.legend()
plt.show()

# Check Tenure distribution
plt.hist(not_churn["Tenure"], label="Not Churn")
plt.hist(churn["Tenure"], label="Churn")
plt.title("Distribution of Tenure")
plt.legend()
plt.show()

# Check Balance distribution
plt.hist(not_churn["Balance"], label="Not Churn")
plt.hist(churn["Balance"], label="Churn")
plt.title("Distribution of Balance")
plt.legend()
plt.show()

# Check CreditScore distribution
plt.hist(not_churn["CreditScore"], label="Not Churn")
plt.hist(churn["CreditScore"], label="Churn")
plt.title("Distribution of Credit Score")
plt.legend()
plt.show()

"""#Data Preprocessing"""

# Correlation Matrix
import seaborn as sns

k = 10 # number of variables for heatmap
df_corr = df.drop(['RowNumber','CustomerId','Surname','Geography','Gender'], axis=1)
cols = df_corr.corr().nlargest(k, 'Exited')['Exited'].index
cm = df[cols].corr()
plt.figure(figsize=(10,6))
sns.heatmap(cm, annot=True, cmap = 'viridis')

"""From the heatmap, we can see that no two numerical variables are highly correlated.
Hence we do not need to drop any numerical variables.
"""

# Check for missing values
df.isnull().sum()

# Drop columns that do not affect the dependent variable
df_prepared = df.drop(['RowNumber','CustomerId','Surname'], axis=1)
df_prepared.head()

"""#Feature Engineering"""

# Encode categorical variables which type are string
from sklearn.preprocessing import LabelEncoder

label_encoder = LabelEncoder()

## Encode Gender
label_encoder.fit(df_prepared['Gender'])
df_prepared['Gender'] = label_encoder.transform(df_prepared['Gender'])

## Encode Geography
label_encoder.fit(df_prepared['Geography'])
df_prepared['Geography'] = label_encoder.transform(df_prepared['Geography'])

df_prepared.head()

# Scale numerical variables using RobustScaler since it handles outliers better
from sklearn.preprocessing import RobustScaler

mmscaler = RobustScaler()
df_prepared[numerical_variables] = mmscaler.fit_transform(df_prepared[numerical_variables])
df_prepared.head()

"""#Modeling"""

# Train-test split
from sklearn.model_selection import train_test_split

X = df_prepared.drop("Exited", axis=1)
y = df_prepared["Exited"]

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

"""#Logistic Regression"""

# Train the model
from sklearn.linear_model import LogisticRegression

model = LogisticRegression()
model.fit(X_train, y_train)

# Make a prediction
y_pred = model.predict(X_test)

# Evaluating the model with Confusion Matrix
from sklearn.metrics import confusion_matrix

cf = confusion_matrix(y_pred, y_test)

print("True Positive : ", cf[1, 1])
print("True Negative : ", cf[0, 0])
print("False Positive: ", cf[0, 1])
print("False Negative: ", cf[1, 0])

# Evaluating the model with performance metrics
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

logreg_accuracy = accuracy_score(y_test, y_pred)
logreg_precision = precision_score(y_test, y_pred)
logreg_recall = recall_score(y_test, y_pred)
logreg_f1 = f1_score(y_test, y_pred)

print("Accuracy: %.2f%%" % (logreg_accuracy * 100.0))
print("Precision: %.2f%%" % (logreg_precision * 100.0))
print("Recall: %.2f%%" % (logreg_recall * 100.0))
print("F1 Score: %.2f%%" % (logreg_f1 * 100.0))

"""#Decision Tree"""

# Train the model
from sklearn.tree import DecisionTreeClassifier

model = DecisionTreeClassifier()
model.fit(X_train, y_train)

# Make a prediction
y_pred = model.predict(X_test)

# Evaluating the model with Confusion Matrix

cf = confusion_matrix(y_pred, y_test)

print("True Positive : ", cf[1, 1])
print("True Negative : ", cf[0, 0])
print("False Positive: ", cf[0, 1])
print("False Negative: ", cf[1, 0])

# Evaluating the model with performance metrics

dt_accuracy = accuracy_score(y_test, y_pred)
dt_precision = precision_score(y_test, y_pred)
dt_recall = recall_score(y_test, y_pred)
dt_f1 = f1_score(y_test, y_pred)

print("Accuracy: %.2f%%" % (dt_accuracy * 100.0))
print("Precision: %.2f%%" % (dt_precision * 100.0))
print("Recall: %.2f%%" % (dt_recall * 100.0))
print("F1 Score: %.2f%%" % (dt_f1 * 100.0))

"""#Random Forest"""

# Train the model
from sklearn.ensemble import RandomForestClassifier

model = RandomForestClassifier()
model.fit(X_train, y_train)

# Make a prediction
y_pred = model.predict(X_test)

# Evaluating the model with Confusion Matrix

cf = confusion_matrix(y_pred, y_test)

print("True Positive : ", cf[1, 1])
print("True Negative : ", cf[0, 0])
print("False Positive: ", cf[0, 1])
print("False Negative: ", cf[1, 0])

# Evaluating the model with performance metrics

rf_accuracy = accuracy_score(y_test, y_pred)
rf_precision = precision_score(y_test, y_pred)
rf_recall = recall_score(y_test, y_pred)
rf_f1 = f1_score(y_test, y_pred)

print("Accuracy: %.2f%%" % (rf_accuracy * 100.0))
print("Precision: %.2f%%" % (rf_precision * 100.0))
print("Recall: %.2f%%" % (rf_recall * 100.0))
print("F1 Score: %.2f%%" % (rf_f1 * 100.0))

"""#XGBoost

"""

# Train the model

from xgboost import XGBClassifier

model = XGBClassifier()
model.fit(X_train, y_train)

# Make a prediction
y_pred = model.predict(X_test)

# Evaluating the model with Confusion Matrix

cf = confusion_matrix(y_pred, y_test)

print("True Positive : ", cf[1, 1])
print("True Negative : ", cf[0, 0])
print("False Positive: ", cf[0, 1])
print("False Negative: ", cf[1, 0])

# Evaluating the model with performance metrics

xg_accuracy = accuracy_score(y_test, y_pred)
xg_precision = precision_score(y_test, y_pred)
xg_recall = recall_score(y_test, y_pred)
xg_f1 = f1_score(y_test, y_pred)

print("Accuracy: %.2f%%" % (xg_accuracy * 100.0))
print("Precision: %.2f%%" % (xg_precision * 100.0))
print("Recall: %.2f%%" % (xg_recall * 100.0))
print("F1 Score: %.2f%%" % (xg_f1 * 100.0))

"""#Comparing Model"""

compare = pd.DataFrame(
    [
        ['Logistic Regression', logreg_accuracy, logreg_precision, logreg_recall, logreg_f1],
        ['Decision Tree', dt_accuracy, dt_precision, dt_recall, dt_f1],
        ['Random Forest', rf_accuracy, rf_precision, rf_recall, rf_f1],
        ['XGBoost', xg_accuracy, xg_precision, xg_recall, xg_f1]
    ],
    columns = ['model', 'accuracy', 'precision', 'recall', 'f1_score']
)

compare.style.highlight_max(color="green")

